var documenterSearchIndex = {"docs":
[{"location":"methods/#Methods-list","page":"Methods","title":"Methods list","text":"","category":"section"},{"location":"methods/","page":"Methods","title":"Methods","text":"","category":"page"},{"location":"methods/#Methods-usage","page":"Methods","title":"Methods usage","text":"","category":"section"},{"location":"methods/","page":"Methods","title":"Methods","text":"Modules = [PERK]","category":"page"},{"location":"methods/#PERK.PERK","page":"Methods","title":"PERK.PERK","text":"PERK\n\nModule implementing parameter estimation via regression with kernels (PERK).\n\nExports\n\nEuclideanKernel: Euclidean inner product kernel (for ridge regression instead of kernel ridge regression)\nGaussianKernel: Gaussian kernel used in kernel ridge regression\nGaussianRFF: Approximation of Gaussian kernel using random Fourier features\ngeneratenoisydata: Function for generating noisy data\nperk: Function for running PERK\n\n\n\n\n\n","category":"module"},{"location":"methods/#PERK.EuclideanKernel","page":"Methods","title":"PERK.EuclideanKernel","text":"EuclideanKernel() <: ExactKernel\n\nCreate a kernel function where the kernel is just the Euclidean inner product (i.e., ridge regression).\n\n\n\n\n\n","category":"type"},{"location":"methods/#PERK.EuclideanKernel-Tuple{AbstractMatrix{<:Real}, AbstractMatrix{<:Real}}","page":"Methods","title":"PERK.EuclideanKernel","text":"(k::EuclideanKernel)(p, q)\n\nEvaluate the Euclidean inner product between p and q.\n\nArguments\n\np::Union{<:Real,<:AbstractVector{<:Real},<:AbstractMatrix{<:Real}}: First kernel input [Q,M] or [M] (if Q = 1) or scalar (if Q = M = 1)\nq::Union{<:Real,<:AbstractVector{<:Real},<:AbstractMatrix{<:Real}}: Second kernel input [Q,N] or [N] (if Q = 1) or scalar (if Q = N = 1)\n\nNote\n\nQ is the number of features\nM is the number of feature vectors in the first input\nN is the number of feature vectors in the second input\n\nReturn\n\nK::Union{<:Real,<:AbstractVector{<:Real},<:AbstractMatrix{<:Real}}: Kernel output [M,N] or [M] (if N = 1) or [N] (if M = 1) or scalar (if M = N = 1)\n\n\n\n\n\n","category":"method"},{"location":"methods/#PERK.ExactKernel","page":"Methods","title":"PERK.ExactKernel","text":"ExactKernel <: Kernel\n\nAbstract type for representing kernels that are evaluated exactly. ExactKernels must be callable with two inputs.\n\n\n\n\n\n","category":"type"},{"location":"methods/#PERK.ExactTrainingData","page":"Methods","title":"PERK.ExactTrainingData","text":"ExactTrainingData(y, x, xm, K, Km, xKinv) <: TrainingData\n\nCreate an object that contains the training data when using the full Gram matrix K.\n\nProperties\n\ny::Union{<:AbstractVector{<:Real},<:AbstractMatrix{<:Real}}: Features for training data [Q,T] or [T] (if Q = 1)\nx::Union{<:AbstractVector{<:Real},<:AbstractMatrix{<:Real}}: Latent parameters for training data [L,T] or [T] (if L = 1)\nxm::Union{<:Real,<:AbstractVector{<:Real}}: Mean of latent parameters [L] or scalar (if L = 1)\nK::AbstractMatrix{<:Real}: De-meaned (both rows and columns) Gram matrix of the kernel evaluated on the training data features [T,T]\nKm::AbstractVector{<:Real}: Row means of K (before de-meaning) [T]\nxKinv::Union{<:AbstractVector{<:Real},<:AbstractMatrix{<:Real}}: x times the regularized inverse of K [L,T] or [T] (if L = 1)\nQ::Integer: Number of training features\nL::Integer: Number of latent parameters\nT::Integer: Number of training points\n\n\n\n\n\n","category":"type"},{"location":"methods/#PERK.GaussianKernel","page":"Methods","title":"PERK.GaussianKernel","text":"GaussianKernel(Λ) <: ExactKernel\n\nCreate a Gaussian kernel function.\n\nProperties\n\nΛ::Union{<:Real,AbstractVector{<:Real}}: Length scales [Q] or scalar (if Q = 1)\n\nNote\n\nQ is the number of features\n\n\n\n\n\n","category":"type"},{"location":"methods/#PERK.GaussianKernel-Tuple{AbstractMatrix{<:Real}, AbstractMatrix{<:Real}}","page":"Methods","title":"PERK.GaussianKernel","text":"(k::GaussianKernel)(p, q)\n\nEvaluate the Gaussian kernel.\n\nArguments\n\np::Union{<:Real,<:AbstractVector{<:Real},<:AbstractMatrix{<:Real}}: First kernel input [Q,M] or [M] (if Q = 1) or scalar (if Q = M = 1)\nq::Union{<:Real,<:AbstractVector{<:Real},<:AbstractMatrix{<:Real}}: Second kernel input [Q,N] or [N] (if Q = 1) or scalar (if Q = N = 1)\n\nNote\n\nQ is the number of features\nM is the number of feature vectors in the first input\nN is the number of feature vectors in the second input\n\nReturn\n\nK::Union{<:Real,<:AbstractVector{<:Real},<:AbstractMatrix{<:Real}}: Kernel output [M,N] or [M] (if N = 1) or [N] (if M = 1) or scalar (if M = N = 1)\n\n\n\n\n\n","category":"method"},{"location":"methods/#PERK.GaussianKernel-Tuple{Union{Complex, AbstractVector{<:Complex}}}","page":"Methods","title":"PERK.GaussianKernel","text":"GuassianKernel(Λy, [Λν])\n\nCreate a Guassian kernel function.\n\nArguments\n\nΛy::Union{<:Number,<:AbstractVector{<:Number}}: Length scales for features [Q] or scalar (if Q = 1)\nΛν::Union{<:Real,<:AbstractVector{<:Real}}: Length scales for known parameters [K] or scalar (if K = 1)\n\n\n\n\n\n","category":"method"},{"location":"methods/#PERK.GaussianRFF","page":"Methods","title":"PERK.GaussianRFF","text":"GaussianRFF(H, Λ) <: RFFKernel\n\nCreate an approximate (via random Fourier features) Gaussian kernel function.\n\nProperties\n\nH::Integer: Approximation order\nΛ::Union{<:Real,AbstractVector{<:Real}}: Length scales [Q] or scalar (if Q = 1)\n\nNote\n\nQ is the number of features\n\n\n\n\n\n","category":"type"},{"location":"methods/#PERK.GaussianRFF-Tuple{Integer, Union{Complex, AbstractVector{<:Complex}}}","page":"Methods","title":"PERK.GaussianRFF","text":"GuassianRFF(H, Λy, [Λν])\n\nCreate an approximate (via random Fourier features) Gaussian kernel function.\n\nArguments\n\nH::Integer: Approximation order\nΛy::Union{<:Number,<:AbstractVector{<:Number}}: Length scales for features [Q] or scalar (if Q = 1)\nΛν::Union{<:Real,<:AbstractVector{<:Real}}: Length scales for known parameters [K] or scalar (if K = 1)\n\n\n\n\n\n","category":"method"},{"location":"methods/#PERK.GaussianRFF-Tuple{Random.AbstractRNG, AbstractMatrix{<:Real}}","page":"Methods","title":"PERK.GaussianRFF","text":"(k::GaussianRFF)([rng], q)\n(k::GaussianRFF)(q, f, phase)\n\nEvaluate the approximate Gaussian kernel.\n\nArguments\n\nrng::AbstractRNG = Random.GLOBAL_RNG: Random number generator to use to generate f and phase\nq::Union{<:Real,<:AbstractVector{<:Real},<:AbstractMatrix{<:Real}}: Kernel input [Q,N] or [N] (if Q = 1) or scalar (if Q = N = 1)\nf::Union{<:AbstractVector{<:Real},<:AbstractMatrix{<:Real}} = randn(k.H, Q): Unscaled random frequency values [H,Q] or [H] (if Q = 1)\nphase::AbstractVector{<:Real} = rand(k.H): Random phase values [H]\n\nNote\n\nQ is the number of features\nN is the number of feature vectors in the input\nH is the approximation order for the random Fourier features\n\nReturn\n\nz::Union{<:AbstractVector{<:Real},<:AbstractMatrix{<:Real}}: Higher-dimensional features [H,N] or [H] (if N = 1)\nfreq::Union{<:AbstractVector{<:Real},<:AbstractMatrix{<:Real}}: Random frequency values [H,Q] or [H] (if Q = 1)\nphase::AbstractVector{<:Real}: Random phase values [H]\n\n\n\n\n\n","category":"method"},{"location":"methods/#PERK.Kernel","page":"Methods","title":"PERK.Kernel","text":"Kernel\n\nAbstract type for representing kernel functions.\n\n\n\n\n\n","category":"type"},{"location":"methods/#PERK.RFFKernel","page":"Methods","title":"PERK.RFFKernel","text":"RFFKernel <: Kernel\n\nAbstract type for representing kernels that are approximated via random Fourier features. RFFKernels must be callable with one or three inputs.\n\n\n\n\n\n","category":"type"},{"location":"methods/#PERK.RFFTrainingData","page":"Methods","title":"PERK.RFFTrainingData","text":"RFFTrainingData(freq, phase, zm, xm, Czz, Cxz, CxzCzzinv) <: TrainingData\n\nCreate an object that contains the training data when using an approximation of the Gram matrix K using random Fourier features.\n\nProperties\n\nfreq::Union{<:AbstractVector{<:Real},<:AbstractMatrix{<:Real}}: Random frequency values for random Fourier features [H,Q] or [H] (if Q = 1)\nphase::AbstractVector{<:Real}: Random phase values for random Fourier features [H]\nzm::AbstractVector{<:Real}: Mean of feature maps [H]\nxm::Union{<:Real,<:AbstractVector{<:Real}}: Mean of latent parameters [L] or scalar (if L = 1)\nCzz::AbstractMatrix{<:Real}: Auto-covariance matrix of feature maps [H,H]\nCxz::Union{<:AbstractVector{<:Real},<:AbstractMatrix{<:Real}}: Cross-covariance matrix between latent parameters and feature maps [L,H] or [H] (if L = 1)\nCxzCzzinv::Union{<:AbstractVector{<:Real},<:AbstractMatrix{<:Real}}: Cxz times the regularized inverse of Czz [L,H] or [H] (if L = 1)\nQ::Integer: Number of training features\nL::Integer: Number of latent parameters\nH::Integer: Kernel approximation order\n\n\n\n\n\n","category":"type"},{"location":"methods/#PERK.TrainingData","page":"Methods","title":"PERK.TrainingData","text":"TrainingData\n\nAbstract type for representing training data.\n\n\n\n\n\n","category":"type"},{"location":"methods/#PERK.addnoise!-Tuple{Any, Any, Any}","page":"Methods","title":"PERK.addnoise!","text":"addnoise!([rng], y, noiseDist)\n\nAdd noise to y. If elements of y are complex-valued, then add independent noise to both the real and imaginary parts of y.\n\n\n\n\n\n","category":"method"},{"location":"methods/#PERK.combine-Tuple{AbstractVector{<:Real}, AbstractVector{<:Real}}","page":"Methods","title":"PERK.combine","text":"combine(y, ν)\n\nCombine the output of the signal models with the known parameters.\n\n\n\n\n\n","category":"method"},{"location":"methods/#PERK.complex2real-Tuple{Complex}","page":"Methods","title":"PERK.complex2real","text":"complex2real(y)\n\nSplit complex data into real and imaginary parts.\n\n\n\n\n\n","category":"method"},{"location":"methods/#PERK.div0-Tuple{Number, Number}","page":"Methods","title":"PERK.div0","text":"div0(a, b)\n\nCompute a / b, but return 0 if b is 0.\n\n\n\n\n\n","category":"method"},{"location":"methods/#PERK.gaussiankernel-Tuple{Any, Any, Any}","page":"Methods","title":"PERK.gaussiankernel","text":"gaussiankernel(p, q, sqrtΣ)\n\nCompute the Gaussian kernel with covariance matrix Σ evaluated at p and q. Note that function input is sqrtΣ, i.e., the square root of Σ.\n\n\n\n\n\n","category":"method"},{"location":"methods/#PERK.generatenoisydata-Tuple{Random.AbstractRNG, Integer, Any, Any, Union{Function, AbstractVector{<:Function}}}","page":"Methods","title":"PERK.generatenoisydata","text":"generatenoisydata([rng], N, xDists, [νDists], noiseDist, signalModels)\n\nGenerate noisy data from unknown (and possibly known) parameter distributions.\n\nArguments\n\nrng::AbstractRNG = Random.GLOBAL_RNG: Random number generator to use\nN::Integer: Number of data points\nxDists: Distributions of latent parameters [L] or scalar (if L = 1); xDists can be any object such that rand(xDists, ::Integer) is defined (or a collection of such objects)\nνDists: Distributions of known parameters [K] or scalar (if K = 1); νDists can be any object such that rand(νDists, ::Integer) is defined (or a collection of such objects); omit this parameter if K = 0\nnoiseDist: Distribution of noise (assumes same noise distribution for both real and imaginary channels in complex case); noiseDist can be any object such that rand(noiseDist, ::Integer) is defined\nsignalModels::Union{<:Function,<:AbstractVector{<:Function}}: Signal models used to generate noiseless data [numSignalModels]; each signal model accepts as inputs L latent parameters (scalars) first, then K known parameters (scalars); user-defined parameters (e.g., scan parameters in MRI) should be built into the signal model\n\nReturn\n\ny::Union{<:AbstractVector{<:Number},<:AbstractMatrix{<:Number}}: Output data of all the (simulated) signals [D,N] or [N] (if D = 1)\nx::Union{<:AbstractVector{<:Real},<:AbstractMatrix{<:Real}}: Randomly generated latent parameters [L,N] or [N] (if L = 1)\nν::Union{<:AbstractVector{<:Real},<:AbstractMatrix{<:Real}}: Randomly generated known parameters [K,N] or [N] (if K = 1); not returned if νDists is omitted\n\n\n\n\n\n","category":"method"},{"location":"methods/#PERK.holdout-Tuple{Random.AbstractRNG, Integer, Integer, AbstractVector{<:Real}, AbstractVector{<:Real}, AbstractVector{<:Real}, AbstractVector, AbstractVector, Any, Union{Function, AbstractVector{<:Function}}, Function}","page":"Methods","title":"PERK.holdout","text":"holdout([rng], N, T, λvals, ρvals, [weights,] xDistsTest, xDistsTrain,\n        noiseDist, signalModels, kernelgenerator; showprogress)\nholdout([rng], N, T, λvals, ρvals, [weights,] xDistsTest, νDistsTest,\n        xDistsTrain, νDistsTrain, noiseDist, signalModels, kernelgenerator;\n        showprogress)\n\nSelect λ and ρ via a holdout process.\n\nArguments\n\nrng::AbstractRNG = Random.GLOBAL_RNG: Random number generator to use\nN::Integer: Number of test points\nT::Integer: Number of training points\nλvals::AbstractVector{<:Real}: Values of λ to search over [nλ]\nρvals::AbstractVector{<:Real}: Values of ρ to search over [nρ]\nweights::AbstractVector{<:Real}: Weights for calculating holdout cost [L]; omit if L = 1\nxDistsTest: Distributions of latent parameters [L] or scalar (if L = 1); xDists can be any object such that rand(xDists, ::Integer) is defined (or a collection of such objects)\nνDistsTest: Distributions of known parameters [K] or scalar (if K = 1); νDists can be any object such that rand(νDists, ::Integer) is defined (or a collection of such objects); omit this parameter if K = 0\nxDistsTrain: Distributions of latent parameters [L] or scalar (if L = 1); xDists can be any object such that rand(xDists, ::Integer) is defined (or a collection of such objects)\nνDistsTrain: Distributions of known parameters [K] or scalar (if K = 1); νDists can be any object such that rand(νDists, ::Integer) is defined (or a collection of such objects); omit this parameter if K = 0\nnoiseDist: Distribution of noise (assumes same noise distribution for both real and imaginary channels in complex case); noiseDist can be any object such that rand(noiseDist, ::Integer) is defined\nsignalModels::Union{<:Function,<:AbstractVector{<:Function}}: Signal models used to generate noiseless data [numSignalModels]; each signal model accepts as inputs L latent parameters (scalars) first, then K known parameters (scalars); user-defined parameters (e.g., scan parameters in MRI) should be built into the signal model\nkernelgenerator::Function: Function that creates a Kernel object given a vector Λ of lengthscales\nshowprogress::Bool = false: Whether to show progress\n\nNote\n\nL is the number of unknown or latent parameters to be estimated\nK is the number of known parameters\nnλ is the number of λ values to try\nnρ is the number of ρ values to try\n\nReturn\n\nλ::Real: Bandwidth scaling parameter\nρ::Real: Tikhonov regularization parameter\nΨ::AbstractMatrix{<:Real}: Holdout costs for λvals and ρvals [nλ,nρ]\n\n\n\n\n\n","category":"method"},{"location":"methods/#PERK.krr-Tuple{Union{Real, AbstractMatrix{<:Real}, AbstractVector{<:Real}}, PERK.ExactTrainingData, PERK.ExactKernel}","page":"Methods","title":"PERK.krr","text":"krr(ytest, trainData, kernel)\n\nPredict latent parameters that generated ytest using kernel ridge regression.\n\nArguments\n\nytest::Union{<:Real,<:AbstractVector{<:Real},<:AbstractMatrix{<:Real}}: Observed test data [Q,N] or [N] (if Q = 1) or scalar (if Q = N = 1)\ntrainData::TrainingData: Training data\nkernel::Kernel: Kernel to use\n\nNotes\n\nQ is the number of observed features per test sample\nN is the number of test samples\n\nReturn\n\nxhat::Union{<:Real,<:AbstractVector{<:Real},<:AbstractMatrix{<:Real}}: Estimated latent parameters [L,N] or [N] (if L = 1) or [L] (if N = 1) or scalar (if L = N = 1)\n\n\n\n\n\n","category":"method"},{"location":"methods/#PERK.krr_train-Tuple{AbstractVector{<:Real}, Union{AbstractMatrix{<:Real}, AbstractVector{<:Real}}, PERK.ExactKernel, Real}","page":"Methods","title":"PERK.krr_train","text":"krr_train([rng], xtrain, ytrain, kernel, ρ)\nkrr_train(xtrain, ytrain, kernel, ρ, f, phase)\n\nTrain kernel ridge regression.\n\nArguments\n\nrng::AbstractRNG = Random.GLOBAL_RNG: Random number generator to use (only used when kernel isa RFFKernel)\nxtrain::Union{<:AbstractVector{<:Real},<:AbstractMatrix{<:Real}}: Latent parameters for training data [L,T] or [T] (if L = 1)\nytrain::Union{<:AbstractVector{<:Real},<:AbstractMatrix{<:Real}}: Features for training data [Q,T] or [T] (if Q = 1)\nkernel::Kernel: Kernel to use\nρ::Real: Tikhonov regularization parameter\nf::Union{<:AbstractVector{<:Real},AbstractMatrix{<:Real}} = randn(kernel.H, Q): Unscaled random frequency values [H,Q] or [H] (if Q = 1) (used when kernel isa RFFKernel)\nphase::AbstractVector{<:Real} = rand(kernel.H): Random phase values [H] (used when kernel isa RFFKernel)\n\nNote\n\nL is the number of unknown or latent parameters to be predicted\nQ is the number of observed features per training sample\nT is the number of training samples\nH is approximation order for kernels that use random Fourier features\n\nReturn\n\ntrainData::TrainingData: TrainingData object to be passed to krr\n\n\n\n\n\n","category":"method"},{"location":"methods/#PERK.perk-Tuple{Random.AbstractRNG, Union{Number, AbstractMatrix{<:Number}, AbstractVector{<:Number}}, Integer, Any, Any, Union{Function, AbstractVector{<:Function}}, PERK.Kernel, Real}","page":"Methods","title":"PERK.perk","text":"perk([rng], y, T, xDists, noiseDist, signalModels, kernel, ρ)\nperk([rng], y, ν, T, xDists, νDists, noiseDist, signalModels, kernel, ρ)\n\nTrain PERK and then estimate latent parameters.\n\nArguments\n\nrng::AbstractRNG = Random.GLOBAL_RNG: Random number generator to use\ny::Union{<:Number,<:AbstractVector{<:Number},<:AbstractMatrix{<:Number}}: Test data points [D,N] or [N] (if D = 1) or scalar (if D = N = 1)\nν::Union{<:Real,<:AbstractVector{<:Real},<:AbstractMatrix{<:Real}}: Known parameters [K,N] or [N] (if K = 1) or scalar (if K = N = 1); omit this parameter if K = 0\nT::Integer: Number of training points\nxDists: Distributions of latent parameters [L] or scalar (if L = 1); xDists can be any object such that rand(xDists, ::Integer) is defined (or a collection of such objects)\nνDists: Distributions of known parameters [K] or scalar (if K = 1); νDists can be any object such that rand(νDists, ::Integer) is defined (or a collection of such objects); omit this parameter if K = 0\nnoiseDist: Distribution of noise (assumes same noise distribution for both real and imaginary channels in complex case); noiseDist can be any object such that rand(noiseDist, ::Integer) is defined\nsignalModels::Union{<:Function,<:AbstractVector{<:Function}}: Signal models used to generate noiseless data [numSignalModels]; each signal model accepts as inputs L latent parameters (scalars) first, then K known parameters (scalars); user-defined parameters (e.g., scan parameters in MRI) should be built into the signal model\nkernel::Kernel: Kernel to use\nρ::Real: Tikhonov regularization parameter\n\nNote\n\nD is the combined number of outputs from all signal models (e.g., number of scans in MRI)\nK is the number of known parameters\nL is the number of unknown or latent parameters to be estimated\nN is the number of test points (e.g., number of voxels in MRI)\n\nReturn\n\nxhat::Union{<:Real,<:AbstractVector{<:Real},<:AbstractMatrix{<:Real}}: Estimated latent parameters [L,N] or [N] (if L = 1) or [L] (if N = 1) or scalar (if L = N = 1)\n\n\n\n\n\n","category":"method"},{"location":"methods/#PERK.perk-Tuple{Union{Number, AbstractMatrix{<:Number}, AbstractVector{<:Number}}, PERK.TrainingData, PERK.Kernel}","page":"Methods","title":"PERK.perk","text":"perk(y, trainData, kernel)\nperk(y, ν, trainData, kernel)\n\nEstimate latent parameters using the provided training data.\n\nArguments\n\ny::Union{<:Number,<:AbstractVector{<:Number},<:AbstractMatrix{<:Number}}: Test data points [D,N] or [N] (if D = 1) or scalar (if D = N = 1)\nν::Union{<:Real,<:AbstractVector{<:Real},<:AbstractMatrix{<:Real}}: Known parameters [K,N] or [N] (if K = 1) or scalar (if K = N = 1); omit this parameter if K = 0\ntrainData::TrainingData: Training data\nkernel::Kernel: Kernel to use\n\nReturn\n\nxhat::Union{<:Real,<:AbstractVector{<:Real},<:AbstractMatrix{<:Real}}: Estimated latent parameters [L,N] or [N] (if L = 1) or [L] (if N = 1) or scalar (if L = N = 1)\n\n\n\n\n\n","category":"method"},{"location":"methods/#PERK.rffmap-Tuple{AbstractMatrix{<:Real}, AbstractMatrix{<:Real}, AbstractVector{<:Real}}","page":"Methods","title":"PERK.rffmap","text":"rffmap(q, freq, phase)\n\nMap features to a higher dimensional space via random Fourier features.\n\nArguments\n\nq::Union{<:Real,<:AbstractVector{<:Real},<:AbstractMatrix{<:Real}}: Lower-dimensional features [Q,N] or [N] (if Q = 1) or scalar (if Q = N = 1)\nfreq::Union{<:AbstractVector{<:Real},<:AbstractMatrix{<:Real}: Random frequency values [H,Q] or [H] (if Q = 1)\nphase::AbstractVector{<:Real}: Random phase values [H]\n\nNote\n\nQ is the number of features\nN is the number of feature vectors in the input\nH is the approximation order for the random Fourier features\n\nReturn\n\nz::Union{<:AbstractVector{<:Real},<:AbstractMatrix{<:Real}}: Higher-dimensional features [H,N] or [H] (if N = 1)\n\n\n\n\n\n","category":"method"},{"location":"methods/#PERK.train-Tuple{Random.AbstractRNG, Integer, Any, Any, Union{Function, AbstractVector{<:Function}}, PERK.Kernel, Real}","page":"Methods","title":"PERK.train","text":"train([rng], T, xDists, [νDists], noiseDist, signalModels, kernel, ρ)\n\nTrain PERK using simulated training data.\n\nArguments\n\nrng::AbstractRNG = Random.GLOBAL_RNG: Random number generator to use\nT::Integer: Number of training points\nxDists: Distributions of latent parameters [L] or scalar (if L = 1); xDists can be any object such that rand(xDists, ::Integer) is defined (or a collection of such objects)\nνDists: Distributions of known parameters [K] or scalar (if K = 1); νDists can be any object such that rand(νDists, ::Integer) is defined (or a collection of such objects); omit this parameter if K = 0\nnoiseDist: Distribution of noise (assumes same noise distribution for both real and imaginary channels in complex case); noiseDist can be any object such that rand(noiseDist, ::Integer) is defined\nsignalModels::Union{<:Function,<:AbstractVector{<:Function}}: Signal models used to generate noiseless data [numSignalModels]; each signal model accepts as inputs L latent parameters (scalars) first, then K known parameters (scalars); user-defined parameters (e.g., scan parameters in MRI) should be built into the signal model\nkernel::Kernel: Kernel to use\nρ::Real: Tikhonov regularization parameter\n\nReturn\n\ntrainData::TrainingData: TrainingData object to be passed to perk\n\n\n\n\n\n","category":"method"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"EditURL = \"https://github.com/StevenWhitaker/PERK.jl/blob/main/docs/lit/examples/docs/lit/examples/01-overview.jl\"","category":"page"},{"location":"generated/examples/01-overview/#overview","page":"PERK overview","title":"PERK overview","text":"","category":"section"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"This page illustrates the Julia package PERK.","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"This page was generated from a single Julia file: 01-overview.jl.","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"In any such Julia documentation, you can access the source code using the \"Edit on GitHub\" link in the top right.","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"The corresponding notebook can be viewed in nbviewer here: 01-overview.ipynb, and opened in binder here: 01-overview.ipynb.","category":"page"},{"location":"generated/examples/01-overview/#Setup","page":"PERK overview","title":"Setup","text":"","category":"section"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"Packages needed here.","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"using PERK: GaussianKernel, krr_train, krr\nusing MIRTjim: jim, prompt\nusing Random: randperm, seed!; seed!(0)\nusing Plots; default(markerstrokecolor = :auto, label=\"\")\nusing InteractiveUtils: versioninfo","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"The following line is helpful when running this file as a script; this way it will prompt user to hit a key after each figure is displayed.","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"isinteractive() ? jim(:prompt, true) : prompt(:draw);\nnothing #hide","category":"page"},{"location":"generated/examples/01-overview/#Overview","page":"PERK overview","title":"Overview","text":"","category":"section"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"Although neural networks are very popular, low-dimensional nonlinear regression problems can be handled quite efficiently by kernel ridge regression (KRR). Training KRR does not require iterative algorithms and is more interpretable than a deep network. It is simply a nonlinear lifting followed by ridge regression.","category":"page"},{"location":"generated/examples/01-overview/#Example","page":"PERK overview","title":"Example","text":"","category":"section"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"Here is an example of using KRR to learn the function y = x^3 from noisy training data.","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"fun(x) = x^3\nNtrain = 101\nxtrain = LinRange(-1, 1, Ntrain) * 3\nytrain = fun.(xtrain) + 1 * randn(size(xtrain))\np0 = scatter(xtrain, ytrain, xlabel=\"x\", ylabel=\"y\", label=\"training data\")\nxlims=(-1,1).*4; ylims=(-1,1).*35\nplot!(p0, fun, label=\"y = x^3\", legend=:top, color=:black; xlims, ylims)","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"Here is the key training step","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"ρ = 1e-5\nλ = 0.5\nkernel = GaussianKernel(λ)\ntrain = krr_train(ytrain, xtrain, kernel, ρ);\nnothing #hide","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"Here is the (demeaned) kernel matrix K","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"jim(train.K, \"PERK K matrix\")","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"Now examine the fit using (exhaustive) test data. The fit is very good within the range of the training data, and regresses to the mean outside of that range.","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"xtest = LinRange(-1, 1, 200) * 4\nyhat = krr(xtest, train, kernel) # todo: remove kernel eventually\np1 = deepcopy(p0)\nplot!(p1, xtest, yhat, label=\"KRR prediction\", color=:magenta)","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"prompt()","category":"page"},{"location":"generated/examples/01-overview/#Parameter-tuning","page":"PERK overview","title":"Parameter tuning","text":"","category":"section"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"PERK has only two tuning parameters: ρ and λ and one can select automatically using cross validation.","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"To illustrate the importance of selecting these parameters properly, here is an example where the regularization parameter ρ is too large, leading to undesirable regression to the mean.","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"λ2, ρ2 = 0.5, 1e-1\nkernel2 = GaussianKernel(λ2)\ntrain2 = krr_train(ytrain, xtrain, kernel2, ρ2);\nyhat2 = krr(xtest, train2, kernel2)\np2 = deepcopy(p0)\nplot!(p2, xtest, yhat2, label=\"KRR prediction\", color=:magenta)","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"prompt()","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"Conversely, here is a case where λ is too small, leading to over-fitting the noisy data.","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"λ3, ρ3 = 1e-1, 1e-5\nkernel3 = GaussianKernel(λ3)\ntrain3 = krr_train(ytrain, xtrain, kernel3, ρ3);\nyhat3 = krr(xtest, train3, kernel3)\np3 = deepcopy(p0)\nplot!(p3, xtest, yhat3, label=\"KRR prediction\", color=:magenta)","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"prompt()","category":"page"},{"location":"generated/examples/01-overview/#Cross-validation","page":"PERK overview","title":"Cross validation","text":"","category":"section"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"One way to apply cross validation to select automatically the two adjustable parameters ρ and λ is to use the holdout function in this package.","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"Cross validation is simple enough to just illustrate directly here.","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"First split the training data into \"fitting\" data and \"validation\" data.","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"Nfit = 70 # use 70% of the data for fitting, 30% for validation\niperm = randperm(Ntrain)\nxfit = xtrain[iperm][1:Nfit]\nyfit = ytrain[iperm][1:Nfit]\nxvalidate = xtrain[iperm][(1+Nfit):end]\nyvalidate = ytrain[iperm][(1+Nfit):end]\np4 = scatter(xfit, yfit;\n    xlabel=\"x\", ylabel=\"y\", label=\"fitting data\", color=:blue)\nscatter!(p4, xvalidate, yvalidate, label=\"validation data\", color=:violet)\nplot!(p4, fun, label=\"y = x^3\", legend=:top, color=:black; xlims, ylims)","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"prompt()","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"Function to evaluate the NRMSE for the validation data for given regularization parameters.","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"function fitmse(ρ, λ)\n    kernel = GaussianKernel(λ)\n    train = krr_train(yfit, xfit, kernel, ρ) # train with \"fit\" data\n    yhat = krr(xvalidate, train, kernel) # test with \"validation\" data\n    return sqrt(sum(abs2, yhat - yvalidate) / sum(abs2, yvalidate)) # NRMSE\nend","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"Use broadcast to evaluate the NRMSE for a grid of ρ,λ values.","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"ρtry = 2. .^ (-32:4)\nλtry = 10 .^ LinRange(-2, 2, 1+2^6)\nfits = fitmse.(ρtry, λtry')\nbest = argmin(fits)\nρbest = ρtry[best[1]]\nλbest = λtry[best[2]]\nl2ρ, l10λ = log2(ρbest), log10(λbest)","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"The best ρ found by CV seems often to be curiously small. There is a fairly wide swath of values having reasonably low validation loss (NRMSE).","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"heatmap(log2.(ρtry), log10.(λtry), fits';\n    title=\"NRMSE\", xlabel=\"log2(ρ)\", ylabel=\"log10(λ)\")\nscatter!([l2ρ], [l10λ], color=:green, marker=:star,\n    label=\"best at log2(ρ)=$l2ρ log10(λ)=$l10λ\")","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"prompt()","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"Profiles through the NRMSE across the minimum. These illustrate that the function is quite non-convex. It is fortunate that there are only two parameters, so that an exhaustive grid search is feasible.","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"p5 = plot(log2.(ρtry), fits[:,best[2]];\n    marker=:circle, ylabel=\"NRMSE\", xlabel=\"log2(ρ)\")\nscatter!([l2ρ], [fits[best]], marker=:star, color=:red)\np6 = plot(log10.(λtry), fits[best[1],:];\n    marker=:circle, ylabel=\"NRMSE\", xlabel=\"log10(λ)\")\nscatter!([l10λ], [fits[best]], marker=:star, color=:red)\nplot(p5, p6, plot_title=\"Profiles\")","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"prompt()","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"Here is the fit with the optimized parameters. The fit is remarkably good and also happens to extrapolate well.","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"kernelb = GaussianKernel(λbest)\ntrainb = krr_train(ytrain, xtrain, kernelb, ρbest);\nyhatb = krr(xtest, trainb, kernelb)\np7 = deepcopy(p0)\nplot!(p7, xtest, yhatb;\n    label=\"KRR prediction after CV\", color=:magenta, ylims=(-1,1).*50)","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"prompt()","category":"page"},{"location":"generated/examples/01-overview/#Reproducibility","page":"PERK overview","title":"Reproducibility","text":"","category":"section"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"This page was generated with the following version of Julia:","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"io = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"And with the following package versions","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"import Pkg; Pkg.status()","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"","category":"page"},{"location":"generated/examples/01-overview/","page":"PERK overview","title":"PERK overview","text":"This page was generated using Literate.jl.","category":"page"},{"location":"","page":"Home","title":"Home","text":"CurrentModule = PERK","category":"page"},{"location":"#PERK.jl-Documentation","page":"Home","title":"PERK.jl Documentation","text":"","category":"section"},{"location":"#Contents","page":"Home","title":"Contents","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"","category":"page"},{"location":"#Overview","page":"Home","title":"Overview","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Skeleton documentation for the Julia package (PERK.jl) that performs parameter estimation with regression with kernels (PERK). See IEEE T-MI paper for details.","category":"page"},{"location":"","page":"Home","title":"Home","text":"For an extensive use of PERK, see this paper on MWF imaging and its associated code.","category":"page"},{"location":"","page":"Home","title":"Home","text":"Also see the \"Examples\".","category":"page"}]
}
