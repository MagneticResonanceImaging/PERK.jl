{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# PERK overview"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This page illustrates the Julia package\n",
    "[`PERK`](https://github.com/StevenWhitaker/PERK.jl).\n",
    "\n",
    "This page was generated from a single Julia file:\n",
    "[01-overview.jl](https://github.com/StevenWhitaker/PERK.jl/blob/main/docs/lit/examples/01-overview.jl)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Setup"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Packages needed here."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "using PERK: GaussianKernel, krr_train, krr\n",
    "using MIRTjim: jim, prompt\n",
    "using Random: randperm, seed!; seed!(0)\n",
    "using Plots; default(markerstrokecolor = :auto, label=\"\")\n",
    "using InteractiveUtils: versioninfo"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following line is helpful when running this file as a script;\n",
    "this way it will prompt user to hit a key after each figure is displayed."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "isinteractive() ? jim(:prompt, true) : prompt(:draw);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Overview"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Although neural networks are very popular,\n",
    "low-dimensional nonlinear regression problems\n",
    "can be handled quite efficiently\n",
    "by kernel ridge regression (KRR).\n",
    "Training KRR does not require iterative algorithms\n",
    "and is more interpretable than a deep network.\n",
    "It is simply a nonlinear lifting\n",
    "followed by ridge regression.\n",
    "\n",
    "\n",
    "### Example\n",
    "\n",
    "Here is an example of using KRR\n",
    "to learn the function $y = x^3$\n",
    "from noisy training data."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "fun(x) = x^3\n",
    "Ntrain = 101\n",
    "xtrain = LinRange(-1, 1, Ntrain) * 3\n",
    "ytrain = fun.(xtrain) + 1 * randn(size(xtrain))\n",
    "p0 = scatter(xtrain, ytrain, xlabel=\"x\", ylabel=\"y\", label=\"training data\")\n",
    "xlims=(-1,1).*4; ylims=(-1,1).*35\n",
    "plot!(p0, fun, label=\"y = x^3\", legend=:top, color=:black; xlims, ylims)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is the key training step"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ρ = 1e-5\n",
    "λ = 0.5\n",
    "kernel = GaussianKernel(λ)\n",
    "train = krr_train(ytrain, xtrain, kernel, ρ);"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is the (demeaned) kernel matrix K"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "jim(train.K, \"PERK K matrix\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now examine the fit using (exhaustive) test data.\n",
    "The fit is very good within the range of the training data,\n",
    "and regresses to the mean outside of that range."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "xtest = LinRange(-1, 1, 200) * 4\n",
    "yhat = krr(xtest, train, kernel) # todo: remove kernel eventually\n",
    "p1 = deepcopy(p0)\n",
    "plot!(p1, xtest, yhat, label=\"KRR prediction\", color=:magenta)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prompt()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Parameter tuning\n",
    "\n",
    "PERK has only two tuning parameters: `ρ` and `λ`\n",
    "and one can select automatically\n",
    "using cross validation.\n",
    "\n",
    "To illustrate the importance of selecting these parameters properly,\n",
    "here is an example where\n",
    "the regularization parameter `ρ` is too large,\n",
    "leading to undesirable regression to the mean."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "λ2, ρ2 = 0.5, 1e-1\n",
    "kernel2 = GaussianKernel(λ2)\n",
    "train2 = krr_train(ytrain, xtrain, kernel2, ρ2);\n",
    "yhat2 = krr(xtest, train2, kernel2)\n",
    "p2 = deepcopy(p0)\n",
    "plot!(p2, xtest, yhat2, label=\"KRR prediction\", color=:magenta)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prompt()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Conversely,\n",
    "here is a case where `λ` is too small,\n",
    "leading to over-fitting the noisy data."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "λ3, ρ3 = 1e-1, 1e-5\n",
    "kernel3 = GaussianKernel(λ3)\n",
    "train3 = krr_train(ytrain, xtrain, kernel3, ρ3);\n",
    "yhat3 = krr(xtest, train3, kernel3)\n",
    "p3 = deepcopy(p0)\n",
    "plot!(p3, xtest, yhat3, label=\"KRR prediction\", color=:magenta)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prompt()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Cross validation\n",
    "\n",
    "One way to apply cross validation\n",
    "to select automatically\n",
    "the two adjustable parameters ρ and λ\n",
    "is to use the `holdout` function in this package.\n",
    "\n",
    "Cross validation is simple enough to just illustrate directly here.\n",
    "\n",
    "First split the training data into \"fitting\" data and \"validation\" data."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "Nfit = 70 # use 70% of the data for fitting, 30% for validation\n",
    "iperm = randperm(Ntrain)\n",
    "xfit = xtrain[iperm][1:Nfit]\n",
    "yfit = ytrain[iperm][1:Nfit]\n",
    "xvalidate = xtrain[iperm][(1+Nfit):end]\n",
    "yvalidate = ytrain[iperm][(1+Nfit):end]\n",
    "p4 = scatter(xfit, yfit;\n",
    "    xlabel=\"x\", ylabel=\"y\", label=\"fitting data\", color=:blue)\n",
    "scatter!(p4, xvalidate, yvalidate, label=\"validation data\", color=:violet)\n",
    "plot!(p4, fun, label=\"y = x^3\", legend=:top, color=:black; xlims, ylims)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prompt()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Function to evaluate the NRMSE for the validation data\n",
    "for given regularization parameters."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "function fitmse(ρ, λ)\n",
    "    kernel = GaussianKernel(λ)\n",
    "    train = krr_train(yfit, xfit, kernel, ρ) # train with \"fit\" data\n",
    "    yhat = krr(xvalidate, train, kernel) # test with \"validation\" data\n",
    "    return sqrt(sum(abs2, yhat - yvalidate) / sum(abs2, yvalidate)) # NRMSE\n",
    "end"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use broadcast to evaluate the NRMSE for a grid of ρ,λ values."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "ρtry = 2. .^ (-32:4)\n",
    "λtry = 10 .^ LinRange(-2, 2, 1+2^6)\n",
    "fits = fitmse.(ρtry, λtry')\n",
    "best = argmin(fits)\n",
    "ρbest = ρtry[best[1]]\n",
    "λbest = λtry[best[2]]\n",
    "l2ρ, l10λ = log2(ρbest), log10(λbest)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "The best ρ found by CV seems often to be curiously small.\n",
    "There is a fairly wide swath of values\n",
    "having reasonably low validation loss (NRMSE)."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "heatmap(log2.(ρtry), log10.(λtry), fits';\n",
    "    title=\"NRMSE\", xlabel=\"log2(ρ)\", ylabel=\"log10(λ)\")\n",
    "scatter!([l2ρ], [l10λ], color=:green, marker=:star,\n",
    "    label=\"best at log2(ρ)=$l2ρ log10(λ)=$l10λ\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prompt()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Profiles through the NRMSE across the minimum.\n",
    "These illustrate that the function is quite non-convex.\n",
    "It is fortunate that there are only two parameters,\n",
    "so that an exhaustive grid search is feasible."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "p5 = plot(log2.(ρtry), fits[:,best[2]];\n",
    "    marker=:circle, ylabel=\"NRMSE\", xlabel=\"log2(ρ)\")\n",
    "scatter!([l2ρ], [fits[best]], marker=:star, color=:red)\n",
    "p6 = plot(log10.(λtry), fits[best[1],:];\n",
    "    marker=:circle, ylabel=\"NRMSE\", xlabel=\"log10(λ)\")\n",
    "scatter!([l10λ], [fits[best]], marker=:star, color=:red)\n",
    "plot(p5, p6, plot_title=\"Profiles\")"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prompt()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "Here is the fit with the optimized parameters.\n",
    "The fit is remarkably good and also happens to extrapolate well."
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "kernelb = GaussianKernel(λbest)\n",
    "trainb = krr_train(ytrain, xtrain, kernelb, ρbest);\n",
    "yhatb = krr(xtest, trainb, kernelb)\n",
    "p7 = deepcopy(p0)\n",
    "plot!(p7, xtest, yhatb;\n",
    "    label=\"KRR prediction after CV\", color=:magenta, ylims=(-1,1).*50)"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "prompt()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Reproducibility"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This page was generated with the following version of Julia:"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "io = IOBuffer(); versioninfo(io); split(String(take!(io)), '\\n')"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "And with the following package versions"
   ],
   "metadata": {}
  },
  {
   "outputs": [],
   "cell_type": "code",
   "source": [
    "import Pkg; Pkg.status()"
   ],
   "metadata": {},
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ],
   "metadata": {}
  }
 ],
 "nbformat_minor": 3,
 "metadata": {
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  },
  "kernelspec": {
   "name": "julia-1.7",
   "display_name": "Julia 1.7.3",
   "language": "julia"
  }
 },
 "nbformat": 4
}
